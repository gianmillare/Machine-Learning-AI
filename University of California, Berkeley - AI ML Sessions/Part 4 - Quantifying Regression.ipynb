{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Regression\n",
    "\n",
    "Focus is to find a way to quanitfy an error in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "# Code used to generate some random data in linear regression models\n",
    "\n",
    "X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model and fit it\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying our Model: Finding our 'Loss'\n",
    "\n",
    "* Mean Squared Error (MSE)\n",
    "* R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11.933040779746149\n",
      "R-Squared: 0.903603363418708\n"
     ]
    }
   ],
   "source": [
    "# We will quantify how well our model does using r2 and MSE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# first use a model to predict\n",
    "predicted = model.predict(X)\n",
    "\n",
    "# Score the prediction with MSE and R2\n",
    "mse = mean_squared_error(y, predicted)\n",
    "r2 = r2_score(y, predicted)\n",
    "\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "print(\"R-Squared: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Mean Squared Error (technical)\n",
    "\n",
    "NOTE: \"(y, predicted)\" - y is our actual (provided) values. and predicted is a result of model.predict(X)  \n",
    "\n",
    "Mean Squared Error - take all y and predicted, subtract them, then square them. Then find the mean of all those squares. This is the average the square between the actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is R Squared?\n",
    "\n",
    "R2 - how much of a variance did the regression capture over the actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom Line:\n",
    "\n",
    "A \"good\" MSE score will be close to zero while a \"good\" R squared score will be close to 1.\n",
    "\n",
    "R squared is the default scoring for many of the Sklearn models.\n",
    "\n",
    "The \"best\" way to test your model is to input your own made up values and checking how accurate / how big of an error you receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.903603363418708"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue...\n",
    "\n",
    "# model.score gives R2\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
